{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5aac146e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86e0698a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic2.0'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "50d57e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22a2a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001F09D7946D0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F09D84CE90> root_client=<openai.OpenAI object at 0x000001F09D797E90> root_async_client=<openai.AsyncOpenAI object at 0x000001F09D7A55D0> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "print(llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f99d3d56",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is Langchain?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\lcenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\lcenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\lcenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\lcenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\lcenv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:689\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    687\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\lcenv\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\lcenv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\lcenv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\lcenv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"What is Langchain?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e71a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"openai/gpt-oss-120b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0cfbb4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Tushar! Nice to meet you. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 78, 'total_tokens': 140, 'completion_time': 0.131011839, 'completion_tokens_details': {'reasoning_tokens': 36}, 'prompt_time': 0.003105225, 'prompt_tokens_details': None, 'queue_time': 0.046098405, 'total_time': 0.134117064}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a09bde29de', 'finish_reason': 'stop', 'logprobs': None}, id='run--9ac815bf-b18f-4433-9e72-954a0fed34e8-0', usage_metadata={'input_tokens': 78, 'output_tokens': 62, 'total_tokens': 140})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hi My name is Tushar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d53caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt Engineering\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [   (\"system\",\"You are a helpful assistant that helps to answer questions related to langchain.\"),\n",
    "        (\"human\",\"{question}\")\n",
    "\n",
    "    ]\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7a3ce1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant that helps to answer questions related to langchain.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f5133e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F09DB8D050>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F09DB8E2D0>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac8c04cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant that helps to answer questions related to langchain.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F09DB8D050>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F09DB8E2D0>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## chaining\n",
    "chain = prompt | model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "15b453eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"question\":\"What is Langgraph?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d64f5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```xml\n",
      "<langsmithInfo>\n",
      "    <summary>\n",
      "        LangSmith is a suite of developer tools designed to help teams build, monitor, and improve applications that use large language models (LLMs). It provides centralized tracing, debugging, testing, and evaluation capabilities, enabling faster iteration and higher reliability for LLM‑driven products.\n",
      "    </summary>\n",
      "    <features>\n",
      "        <feature>\n",
      "            <name>Tracing &amp; Logging</name>\n",
      "            <description>Automatic capture of every LLM call, including prompts, responses, metadata, and execution timestamps, all viewable in a searchable UI.</description>\n",
      "        </feature>\n",
      "        <feature>\n",
      "            <name>Debugging</name>\n",
      "            <description>Step‑through inspection of prompt templates, token usage, and model parameters to pinpoint issues quickly.</description>\n",
      "        </feature>\n",
      "        <feature>\n",
      "            <name>Testing &amp; Evaluation</name>\n",
      "            <description>Define test suites with expected outputs, run them against multiple model versions, and track performance regressions.</description>\n",
      "        </feature>\n",
      "        <feature>\n",
      "            <name>Analytics &amp; Metrics</name>\n",
      "            <description>Built‑in dashboards for latency, cost, token consumption, and success rates, with exportable reports.</description>\n",
      "        </feature>\n",
      "        <feature>\n",
      "            <name>Collaboration</name>\n",
      "            <description>Share traces, annotations, and test results across teams, with role‑based access controls.</description>\n",
      "        </feature>\n",
      "        <feature>\n",
      "            <name>Integrations</name>\n",
      "            <description>SDKs for Python, JavaScript, and other languages; native hooks for popular frameworks such as LangChain and LlamaIndex.</description>\n",
      "        </feature>\n",
      "    </features>\n",
      "    <useCases>\n",
      "        <useCase>\n",
      "            <title>Prompt Engineering</title>\n",
      "            <description>Iteratively refine prompts by reviewing historical traces and comparing model outputs.</description>\n",
      "        </useCase>\n",
      "        <useCase>\n",
      "            <title>Model Monitoring in Production</title>\n",
      "            <description>Detect drift or anomalies in live applications by monitoring latency, error rates, and token usage.</description>\n",
      "        </useCase>\n",
      "        <useCase>\n",
      "            <title>Automated Regression Testing</title>\n",
      "            <description>Run a suite of predefined queries against new model releases to ensure no degradation in quality.</description>\n",
      "        </useCase>\n",
      "    </useCases>\n",
      "    <website>https://www.langsmith.com</website>\n",
      "</langsmithInfo>\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "af55ddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangSmith** is the **observability, debugging, testing, and evaluation platform** built by the creators of LangChain for developers who build applications that use large language models (LLMs).  \n",
      "\n",
      "---\n",
      "\n",
      "## TL;DR\n",
      "- **What it is:** A SaaS service that records every step of an LLM‑driven workflow (prompts, responses, metadata, custom logs) and gives you a UI + APIs to explore, compare, and improve those runs.  \n",
      "- **Why you need it:** LLM apps are often “black boxes.” LangSmith turns them into observable, reproducible pipelines so you can spot bugs, reduce hallucinations, measure performance, and iterate faster.  \n",
      "- **Core pieces:** Tracing, Evaluation, Prompt Management, and Collaboration.  \n",
      "- **How you use it:** Add a few lines of code (or a decorator) to your LangChain (or any Python) workflow, push runs to the LangSmith server, then view them in the web UI or query them via the SDK.  \n",
      "\n",
      "---\n",
      "\n",
      "## 1. The problem LangSmith solves\n",
      "\n",
      "| Challenge in LLM apps | What happens without observability |\n",
      "|-----------------------|------------------------------------|\n",
      "| **Prompt drift / hallucinations** | You can’t tell why a bad answer was produced. |\n",
      "| **Cost overruns** | No visibility into token usage per request. |\n",
      "| **Hard‑to‑reproduce bugs** | Errors appear only in production, with no logs. |\n",
      "| **Model upgrades** | You can’t compare old vs. new model behavior at scale. |\n",
      "| **Team hand‑off** | No shared place to store prompts, test cases, or evaluation results. |\n",
      "\n",
      "LangSmith gives you a single source of truth for all of the above.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core features\n",
      "\n",
      "| Feature | What it does | Typical use‑case |\n",
      "|---------|--------------|------------------|\n",
      "| **Run tracing** | Every call to a LLM, tool, or chain is recorded: prompt, response, token usage, timestamps, environment info, custom metadata. | Debug a single request that produced a hallucination. |\n",
      "| **Prompt versioning** | Store each prompt (or chain) with a unique ID and see its evolution over time. | Track how a “few‑shot” prompt changes after a rewrite. |\n",
      "| **Evaluation suite** | Define test cases (input → expected output) and run them automatically; metrics include exact match, BLEU, ROUGE, custom functions, and cost. | Continuous integration for your chatbot. |\n",
      "| **Comparison dashboard** | Side‑by‑side view of runs from different models, temperatures, or prompt versions, with diffed outputs. | Decide whether to upgrade from `gpt‑3.5‑turbo` to `gpt‑4`. |\n",
      "| **Cost & latency analytics** | Aggregate token counts, API cost, and latency per run, per model, per chain. | Spot a sudden spike in token usage after a prompt change. |\n",
      "| **Alerting & monitoring** | Set thresholds on metrics (e.g., “average hallucination score > 0.3”) and receive Slack/Email alerts. | Early warning when a downstream API starts failing. |\n",
      "| **Collaboration** | Share runs, annotate them, comment, and tag teammates. | Review a QA engineer’s test suite together with the data‑science team. |\n",
      "| **API & SDK** | Python SDK (`langsmith`), REST API, and LangChain integration (`langchain.callbacks.langsmith`). | Use LangSmith data in custom dashboards or CI pipelines. |\n",
      "| **Privacy & compliance** | Runs can be stored in a private VPC, encrypted at rest, and you can redact PII fields via SDK hooks. | Enterprise customers with GDPR/CCPA constraints. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. How you integrate it (Python example)\n",
      "\n",
      "```python\n",
      "# 1️⃣ Install the packages\n",
      "pip install langchain langsmith\n",
      "\n",
      "# 2️⃣ Set your LangSmith API key (env var or .env)\n",
      "import os\n",
      "os.environ[\"LANGSMITH_API_KEY\"] = \"ls_XXXXXXXXXXXXXXXX\"\n",
      "\n",
      "# 3️⃣ Enable tracing – either globally or per‑chain\n",
      "from langchain import OpenAI, LLMChain, PromptTemplate\n",
      "from langchain.callbacks.langsmith import LangSmithCallbackHandler\n",
      "\n",
      "# Global handler (captures everything)\n",
      "handler = LangSmithCallbackHandler()\n",
      "# Or use a context manager for a specific block:\n",
      "# with handler.tracing_context():\n",
      "#     ...\n",
      "\n",
      "# 4️⃣ Build a simple chain\n",
      "prompt = PromptTemplate.from_template(\"Translate to French: {text}\")\n",
      "llm = OpenAI(model=\"gpt-4o-mini\")\n",
      "chain = LLMChain(prompt=prompt, llm=llm, callbacks=[handler])\n",
      "\n",
      "# 5️⃣ Run it – the run is automatically pushed to LangSmith\n",
      "result = chain.invoke({\"text\": \"Hello, how are you?\"})\n",
      "print(result[\"text\"])\n",
      "```\n",
      "\n",
      "*What happens under the hood*  \n",
      "\n",
      "- The `LangSmithCallbackHandler` intercepts the `on_chain_start`, `on_llm_start`, `on_llm_end`, etc., events generated by LangChain.  \n",
      "- It packages the prompt, response, token usage, timestamps, and any user‑provided metadata into a **Run** object.  \n",
      "- The SDK posts the Run to `https://api.smith.langchain.com/v1/runs`.  \n",
      "- In the web UI you’ll see a tree view: `Chain → Prompt → LLM → Output`, each node expandable with raw JSON.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Evaluation workflow\n",
      "\n",
      "```python\n",
      "from langsmith import evaluate, EvaluationResult\n",
      "from langsmith.evaluation import Criteria\n",
      "\n",
      "# Define a simple correctness criterion\n",
      "def exact_match(prediction, reference):\n",
      "    return prediction.strip() == reference.strip()\n",
      "\n",
      "criteria = [\n",
      "    Criteria(name=\"ExactMatch\", metric=exact_match, higher_is_better=True)\n",
      "]\n",
      "\n",
      "# A list of test cases\n",
      "test_cases = [\n",
      "    {\"input\": \"2+2\", \"expected\": \"4\"},\n",
      "    {\"input\": \"Capital of France?\", \"expected\": \"Paris\"},\n",
      "]\n",
      "\n",
      "# Run the evaluation (LangChain will invoke the chain for each case)\n",
      "results = evaluate(\n",
      "    chain=chain,\n",
      "    inputs=[c[\"input\"] for c in test_cases],\n",
      "    expected_outputs=[c[\"expected\"] for c in test_cases],\n",
      "    criteria=criteria,\n",
      ")\n",
      "\n",
      "# `results` is a list of EvaluationResult objects; they are also stored in LangSmith.\n",
      "for r in results:\n",
      "    print(r.run_id, r.scores)\n",
      "```\n",
      "\n",
      "The UI then shows a **Test Suite** page with per‑case scores, aggregate metrics, and a heat‑map of failures.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Typical workflows where LangSmith shines\n",
      "\n",
      "| Workflow | How LangSmith helps |\n",
      "|----------|---------------------|\n",
      "| **Rapid prototyping** | Instantly view the prompt you sent, the raw LLM response, and token usage without adding manual logging. |\n",
      "| **Production monitoring** | Set alerts on latency spikes or on a custom “hallucination score” you compute in a callback. |\n",
      "| **Model migration** | Run the same test suite against `gpt‑3.5‑turbo`, `gpt‑4`, and a self‑hosted Llama model; compare cost vs. quality in the Comparison dashboard. |\n",
      "| **Team hand‑off** | Export runs as JSON, attach them to JIRA tickets, or embed them in Confluence pages for non‑technical stakeholders. |\n",
      "| **Compliance** | Redact or omit sensitive fields before sending runs; LangSmith can be hosted in a private VPC for regulated industries. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Pricing & availability (as of early 2025)\n",
      "\n",
      "| Tier | Key limits | Typical audience |\n",
      "|------|------------|------------------|\n",
      "| **Free** | 5 k runs / month, 30 days retention, community support | Hobbyists, early‑stage prototypes |\n",
      "| **Pro** | 100 k runs / month, 90 days retention, custom alerts, priority support | Small‑to‑medium SaaS products |\n",
      "| **Enterprise** | Unlimited runs, VPC/private cloud, SSO/SAML, dedicated account manager, SLA | Large organizations, regulated sectors |\n",
      "\n",
      "> *Note*: Pricing can evolve; always check the latest on the LangSmith website.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. How LangSmith fits into the broader LangChain ecosystem\n",
      "\n",
      "| Component | Role |\n",
      "|-----------|------|\n",
      "| **LangChain** | The library you use to compose LLM calls, tools, agents, and memory. |\n",
      "| **LangSmith** | The observability layer that *records* everything LangChain does (and can also be used with any plain Python LLM call). |\n",
      "| **LangServe** | A lightweight API server for LangChain apps. When you deploy a LangServe endpoint, you can enable LangSmith tracing with a single env var (`LANGSMITH_TRACING_ENABLED=1`). |\n",
      "| **LangGraph** | For state‑machine workflows; LangSmith automatically captures each node transition as a sub‑run, giving you a visual graph of execution. |\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Quick checklist to get started\n",
      "\n",
      "1. **Create a LangSmith account** → obtain an API key.  \n",
      "2. **Add the SDK** (`pip install langsmith`).  \n",
      "3. **Insert the callback handler** into your LangChain objects (or use the `@langsmith.traceable` decorator).  \n",
      "4. **Run a few requests** → verify they appear in the UI.  \n",
      "5. **Define at least one evaluation test suite** → start tracking quality over time.  \n",
      "6. **Set up alerts** for cost/latency/hallucination thresholds.  \n",
      "7. **Invite teammates** and start collaborating on runs.\n",
      "\n",
      "---\n",
      "\n",
      "## 9. Frequently asked questions\n",
      "\n",
      "| Question | Answer |\n",
      "|----------|--------|\n",
      "| *Can I use LangSmith without LangChain?* | Yes. The SDK works with any Python function that returns a string or JSON; you just wrap the function with `@langsmith.traceable`. |\n",
      "| *Does LangSmith store the raw user data I send to the LLM?* | By default it stores the full prompt and response. You can configure redaction hooks (`handler.on_llm_start = lambda ...: {\"prompt\": redact(prompt)}`) or disable storing of certain fields. |\n",
      "| *Is there a limit on model providers?* | No. It works with OpenAI, Anthropic, Cohere, Azure, Bedrock, locally hosted models (e.g., Llama‑cpp), or any endpoint you call via an HTTP client. |\n",
      "| *Can I query runs programmatically?* | Yes – the REST API supports filtering by `run_id`, `tags`, `start_time`, `metadata`, etc. The Python SDK provides `langsmith.Client().list_runs(...)`. |\n",
      "| *How does LangSmith differ from generic APM tools (Datadog, New Relic)?* | Those tools monitor system‑level metrics (CPU, latency, errors) but lack LLM‑specific context (prompt, token usage, model version). LangSmith is purpose‑built for LLM pipelines, giving you the **semantic** observability you need. |\n",
      "\n",
      "---\n",
      "\n",
      "## 10. TL;DR recap\n",
      "\n",
      "- **LangSmith** = observability platform for LLM‑driven apps.  \n",
      "- Captures **prompts, responses, metadata, cost, latency**, and lets you **search, compare, evaluate, and alert** on them.  \n",
      "- Integrated out‑of‑the‑box with **LangChain**, but works with any Python code.  \n",
      "- Enables **debugging, continuous evaluation, cost control, and team collaboration**—the missing pieces for production‑grade LLM applications.\n",
      "\n",
      "If you’re building anything beyond a quick demo—chatbots, agents, retrieval‑augmented generation, or internal decision‑support tools—adding LangSmith early will save you countless hours of “why did the model do that?” debugging later. Happy tracing!\n"
     ]
    }
   ],
   "source": [
    "### outputparser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"question\":\"What is Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1aac349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt Engineering2\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [   (\"system\",\"ou are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"human\",\"{question}\")\n",
    "\n",
    "    ]\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d764661d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ou are an expert AI Engineer.Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce56f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"question\": \"What is Langsmith?\",\n",
      "  \"answer\": \"LangSmith is a developer platform and suite of tools created by LangChain that helps users build, test, monitor, and debug LLM (large language model) applications. It provides features such as:\\n- **Tracing and logging** of LLM calls, prompts, and responses for visibility into model behavior.\\n- **Evaluation and testing** utilities to compare model outputs against expected results or benchmarks.\\n- **Prompt management** for versioning, storing, and reusing prompts across projects.\\n- **Performance monitoring** with metrics like latency, token usage, and cost.\\n- **Collaboration** tools that allow teams to share runs, experiments, and insights.\\nLangSmith integrates with the LangChain ecosystem, enabling developers to instrument their LangChain pipelines with minimal code changes, thereby simplifying the development lifecycle of AI-powered applications.\",\n",
      "  \"source\": \"LangChain documentation and product overview (as of 2024)\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "response = chain.invoke({\"question\":\"What is Langsmith?\"}) \n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7484050a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt Engineering3\n",
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d4e0658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```xml\\n<langsmithInfo>\\n    <overview>\\n        Langsmith is a platform designed to help developers and data scientists manage, monitor, and improve the performance of large language model (LLM) applications. It provides tools for tracing, evaluating, and versioning prompts, model outputs, and associated metadata, enabling teams to iterate quickly and maintain high-quality AI services.\\n    </overview>\\n    <features>\\n        <feature>\\n            <name>Tracing &amp; Logging</name>\\n            <description>Automatically captures inputs, outputs, and execution metadata for every LLM call, allowing detailed inspection of model behavior.</description>\\n        </feature>\\n        <feature>\\n            <name>Evaluation Framework</name>\\n            <description>Supports custom evaluation metrics and benchmark datasets to assess response quality, relevance, and safety.</description>\\n        </feature>\\n        <feature>\\n            <name>Prompt Management</name>\\n            <description>Version controls prompts, stores prompt templates, and tracks changes over time.</description>\\n        </feature>\\n        <feature>\\n            <name>Dashboard &amp; Analytics</name>\\n            <description>Provides visual dashboards for latency, error rates, token usage, and other key performance indicators.</description>\\n        </feature>\\n        <feature>\\n            <name>Collaboration</name>\\n            <description>Allows team members to share experiments, annotate results, and comment on model runs.</description>\\n        </feature>\\n        <feature>\\n            <name>Integration</name>\\n            <description>Offers SDKs and API endpoints for seamless integration with popular LLM providers such as OpenAI, Anthropic, Cohere, and custom hosted models.</description>\\n        </feature>\\n    </features>\\n    <useCases>\\n        <useCase>\\n            <title>Prompt Optimization</title>\\n            <description>Iteratively test variations of prompts, compare metrics, and select the most effective version.</description>\\n        </useCase>\\n        <useCase>\\n            <title>Model Monitoring in Production</title>\\n            <description>Detect drift, latency spikes, or unexpected outputs in real‑time deployments.</description>\\n        </useCase>\\n        <useCase>\\n            <title>Compliance &amp; Auditing</title>\\n            <description>Maintain a complete audit trail of all LLM interactions for regulatory review.</description>\\n        </useCase>\\n        <useCase>\\n            <title>Team Collaboration</title>\\n            <description>Share experiment results across engineers, product managers, and researchers to align on model performance goals.</description>\\n        </useCase>\\n    </useCases>\\n    <gettingStarted>\\n        <step>1. Sign up for a Langsmith account at https://www.langsmith.com.</step>\\n        <step>2. Install the SDK: <code>pip install langsmith</code>.</step>\\n        <step>3. Initialize the client with your API key:\\n            <code>\\n                import langsmith\\n                client = langsmith.Client(api_key=\"YOUR_API_KEY\")\\n            </code>\\n        </step>\\n        <step>4. Wrap your LLM calls with the tracing decorator:\\n            <code>\\n                @client.trace\\n                def generate(prompt):\\n                    return openai.ChatCompletion.create(model=\"gpt-4\", messages=[{\"role\":\"user\",\"content\":prompt}])\\n            </code>\\n        </step>\\n        <step>5. Visit the Langsmith dashboard to explore logs, run evaluations, and refine prompts.</step>\\n    </gettingStarted>\\n</langsmithInfo>\\n```' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 920, 'prompt_tokens': 237, 'total_tokens': 1157, 'completion_time': 1.950781379, 'completion_tokens_details': {'reasoning_tokens': 191}, 'prompt_time': 0.015144488, 'prompt_tokens_details': None, 'queue_time': 0.053222153, 'total_time': 1.9659258670000002}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_e10890e4b9', 'finish_reason': 'stop', 'logprobs': None} id='run--7c43a694-6ac2-4b27-b67d-9d1b3ab2f0cf-0' usage_metadata={'input_tokens': 237, 'output_tokens': 920, 'total_tokens': 1157}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0a25063e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything.'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "#from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "#model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "158fad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'Why did the scarecrow win an award? Because he was outstanding in his field!'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "08ccd9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Big (1988)</movie>\n",
      "<movie>Philadelphia (1993)</movie>\n",
      "<movie>Forrest Gump (1994)</movie>\n",
      "<movie>Saving Private Ryan (1998)</movie>\n",
      "<movie>Cast Away (2000)</movie>\n",
      "<movie>Road to Perdition (2002)</movie>\n",
      "<movie>Catch Me If You Can (2002)</movie>\n",
      "<movie>The Terminal (2004)</movie>\n",
      "<movie>Bridge of Spies (2015)</movie>\n",
      "<movie>Captain Phillips (2013)</movie>\n",
      "<movie>Saving Mr. Banks (2013)</movie>\n",
      "<movie>Sully (2016)</movie>\n",
      "<movie>The Post (2017)</movie>\n",
      "<movie>Greyhound (2020)</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7cc6a3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the scarecrow win an award?', punchline='Because he was outstanding in his field.')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "#from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "#model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f7caf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcenv (Python 3.11)",
   "language": "python",
   "name": "lcenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
